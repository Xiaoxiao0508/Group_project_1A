{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "# step1 loading data,check null value\n",
    "player=pd.read_csv('2010-11.csv',sep=',')\n",
    "player.info()\n",
    "player.isnull().sum()\n",
    "# step 2, processing data,set up unique labels and group names,use labelEncoder to set up the ratio to 0,1,\n",
    "# use sns to shrew it into  a graph\n",
    "# may play with the bins and make the bins lean more towards the high win ratio\n",
    "bins=(-0.1,0.5,1.0)\n",
    "group_names=['lose','win']\n",
    "player['W_PCT']=pd.cut(player['W_PCT'], bins=bins,labels=group_names)\n",
    "player['W_PCT'].unique()\n",
    "label_ratio=LabelEncoder()\n",
    "player['W_PCT']=label_ratio.fit_transform(player['W_PCT'].astype(str))\n",
    "player.head(20)\n",
    "player['W_PCT'].value_counts()\n",
    "sns.countplot(x=player['W_PCT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3, seperate the dataset as response variable and feature variables, get ready for model\n",
    "X=player.drop('W_PCT',axis=1)\n",
    "y=player['W_PCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4,Use train_test_split(package) to split the data to  Train data and Test data, default test_size 25%\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5 ,Scale up the data Applying Standart scaling to get optimized result eg, big number overweight the impact of small number\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "# X_test[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6,choose classifier\n",
    "# RANDOM FOREST -Least amount of parts to fine-tune,\n",
    "# used for a medium sized data set\n",
    "# 1,create randomForest variable,2,fit the training data to it,3 predict\n",
    "rfc=RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train,y_train)\n",
    "pred_rfc=rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test[:20]\n",
    "# the test data print out have been sacled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rfc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the performence of the model\n",
    "print(classification_report(y_test,pred_rfc))\n",
    "print(confusion_matrix(y_test,pred_rfc))\n",
    "# the consusion matrix report shows a lot of miss lable of 'win'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier\n",
    "# clf=svm.SVC()\n",
    "# clf.fit(X_train,y_train)\n",
    "# pred_clf=clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performence of model\n",
    "# print(classification_report(y_test,pred_clf))\n",
    "# print(confusion_matrix(y_test,pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network classifier\n",
    "# works well with huge amount of data\n",
    "# mlpc=MLPClassifier(hidden_layer_sizes=(11,11,11),max_iter=500)\n",
    "# mlpc.fit(X_train,y_train)\n",
    "# pred_mlpc=mlpc.predict(X_test)\n",
    "# print(classification_report(y_test,pred_mlpc))\n",
    "# print(confusion_matrix(y_test,pred_mlpc))\n",
    "# A convergence point is a machine learning models localized optimal state. \n",
    "# It basically means that the variables within the model have the best posible values in order to\n",
    "# predict a target feature based on another set of features. In MLP, these variables are the weights within each neuron. \n",
    "# Generally, when a data set doesn't represent a organized and discernable pattern, machine learning algorithms might not be able to find a convergence point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step, use the choosen model and given new feature data to predict \n",
    "Xnew=[[20,18,3.6,9.9,0.36,1.2,2,0.28,2,2,0.7,0.2,0,1,2,1,0.5,0.1,0.5,1.3,2,9,-2,15,0,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-603793461839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mXnew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mynew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mynew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "Xnew=sc.transform(Xnew)\n",
    "ynew=rfc.predict(Xnew)\n",
    "ynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
